{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/\"\n",
    "sp_data_path = data_path + \"SP-train.npy\"\n",
    "wp_data_path = data_path + \"WP-train.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qianqi/.conda/envs/nlp243/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# # do this once\n",
    "# np_load_old = np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_train = np.load(sp_data_path)\n",
    "wp_train = np.load(wp_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "507\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "print(len(sp_train))\n",
    "print(len(wp_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"bert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_data_sp, val_data_sp = train_test_split(sp_train, test_size=0.1, random_state = 17)\n",
    "train_data_wp, val_data_wp = train_test_split(wp_train, test_size=0.1, random_state = 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sp_wp(sp, wp):\n",
    "    combined_p = [x for x in sp]\n",
    "    for x in wp:\n",
    "        combined_p.append(x)\n",
    "    return combined_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = combine_sp_wp(train_data_sp, train_data_wp)\n",
    "val_data = combine_sp_wp(val_data_sp, val_data_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "812\n",
      "{'id': 'SP-196_SR', 'question': 'A tank contains ten fish. Two fish drown. Four fish are swimming away.Three fish perish.\\xa0How many fish are there left in the tank?', 'answer': 'None of above.', 'distractor1': 'Seven.', 'distractor2': 'Twenty.', 'distractor(unsure)': 'Five.', 'label': 3, 'choice_list': ['Seven.', 'Twenty.', 'Five.', 'None of above.'], 'choice_order': [1, 2, 3, 0]}\n",
      "{'id': 'WP-46', 'question': 'What 4 days in the week start with T?', 'answer': 'Tuesday, Thursday, Today and Tomorrow.', 'distractor1': 'Monday, Tuesday, Wednesday and Thursday.', 'distractor2': 'Tuesday, Wednesday,Thursday and Friday.', 'distractor(unsure)': 'None of above.', 'label': 1, 'choice_list': ['Monday, Tuesday, Wednesday and Thursday.', 'Tuesday, Thursday, Today and Tomorrow.', 'Tuesday, Wednesday,Thursday and Friday.', 'None of above.'], 'choice_order': [1, 0, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(train_data[0])\n",
    "print(train_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(data):\n",
    "    # Repeat each quesiton four times to go with the four possibilities of second sentences.\n",
    "    questions = [[i[\"question\"]] * 4 for i in data]\n",
    "    # Grab all choices possible for each context.\n",
    "    choices = [i[\"choice_list\"] for i in data]\n",
    "    # Flatten everything\n",
    "    questions = sum(questions, [])\n",
    "    choices = sum(choices, [])\n",
    "    # Tokenize\n",
    "    tokenized_qa = tokenizer(questions, choices, truncation=True, padding=True)\n",
    "    # Un-flatten\n",
    "    return {k: [v[i : i+4] for i in range(0, len(v), 4)] for k, v in tokenized_qa.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['A tank contains ten fish. Two fish drown. Four fish are swimming away.Three fish perish.\\xa0How many fish are there left in the tank?', 'A tank contains ten fish. Two fish drown. Four fish are swimming away.Three fish perish.\\xa0How many fish are there left in the tank?', 'A tank contains ten fish. Two fish drown. Four fish are swimming away.Three fish perish.\\xa0How many fish are there left in the tank?', 'A tank contains ten fish. Two fish drown. Four fish are swimming away.Three fish perish.\\xa0How many fish are there left in the tank?'], ['I break down walls and devour towers. I consume iron and corrode steel, but still, I am essential to all. Sometimes, people desire me, yet they fear my uncontrolled presence. What am I?', 'I break down walls and devour towers. I consume iron and corrode steel, but still, I am essential to all. Sometimes, people desire me, yet they fear my uncontrolled presence. What am I?', 'I break down walls and devour towers. I consume iron and corrode steel, but still, I am essential to all. Sometimes, people desire me, yet they fear my uncontrolled presence. What am I?', 'I break down walls and devour towers. I consume iron and corrode steel, but still, I am essential to all. Sometimes, people desire me, yet they fear my uncontrolled presence. What am I?'], ['One morning a man is leaving on a business trip and finds he left some paperwork at his office. He runs into his office to get it and the night watchman stops him and says, \"Sir, don\\'t get on the plane. I had a dream last night that the plane would crash and everyone would die!\" The man takes his word and cancels his trip. Sure enough, the plane crashes and everyone dies. The next morning the man gives the watchman a $1,000 reward for saving his life and then fires him. Why did he fire the watchman that saved his life?', 'One morning a man is leaving on a business trip and finds he left some paperwork at his office. He runs into his office to get it and the night watchman stops him and says, \"Sir, don\\'t get on the plane. I had a dream last night that the plane would crash and everyone would die!\" The man takes his word and cancels his trip. Sure enough, the plane crashes and everyone dies. The next morning the man gives the watchman a $1,000 reward for saving his life and then fires him. Why did he fire the watchman that saved his life?', 'One morning a man is leaving on a business trip and finds he left some paperwork at his office. He runs into his office to get it and the night watchman stops him and says, \"Sir, don\\'t get on the plane. I had a dream last night that the plane would crash and everyone would die!\" The man takes his word and cancels his trip. Sure enough, the plane crashes and everyone dies. The next morning the man gives the watchman a $1,000 reward for saving his life and then fires him. Why did he fire the watchman that saved his life?', 'One morning a man is leaving on a business trip and finds he left some paperwork at his office. He runs into his office to get it and the night watchman stops him and says, \"Sir, don\\'t get on the plane. I had a dream last night that the plane would crash and everyone would die!\" The man takes his word and cancels his trip. Sure enough, the plane crashes and everyone dies. The next morning the man gives the watchman a $1,000 reward for saving his life and then fires him. Why did he fire the watchman that saved his life?'], [\"Jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. When he got back, he didn't get any medals or any praises. Why?\", \"Jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. When he got back, he didn't get any medals or any praises. Why?\", \"Jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. When he got back, he didn't get any medals or any praises. Why?\", \"Jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. When he got back, he didn't get any medals or any praises. Why?\"], ['A woman living in Rome legally gave birth to five children, but she has never been pregnant. How is this possible?', 'A woman living in Rome legally gave birth to five children, but she has never been pregnant. How is this possible?', 'A woman living in Rome legally gave birth to five children, but she has never been pregnant. How is this possible?', 'A woman living in Rome legally gave birth to five children, but she has never been pregnant. How is this possible?']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"[CLS] jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. when he got back, he didn't get any medals or any praises. why? [SEP] jimy was the name of the fighter - jet that was flying over enemy's territory. [SEP]\",\n",
       " \"[CLS] jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. when he got back, he didn't get any medals or any praises. why? [SEP] jimy was really sweaty and smells. [SEP]\",\n",
       " \"[CLS] jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. when he got back, he didn't get any medals or any praises. why? [SEP] jimy is could not lift the trophy because it was too heavy. [SEP]\",\n",
       " \"[CLS] jimy went in the middle of the warzone and bombarded a lot of territories that were in the possession of the enemy. when he got back, he didn't get any medals or any praises. why? [SEP] none of above. [SEP]\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = train_data[:5]\n",
    "# 'input_ids', 'token_type_ids', 'attention_mask', (#data, 4, #feature)\n",
    "features = preprocess_function(examples) \n",
    "idx = 3\n",
    "[tokenizer.decode(features[\"input_ids\"][idx][i]) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [[{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Un-flatten\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        # Add back labels\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_keys = [\"input_ids\", \"attention_mask\"]\n",
    "train_features = preprocess_function(train_data) \n",
    "for i in range(len(train_data)):\n",
    "    for k in accepted_keys:\n",
    "        train_data[i][k] = train_features[k][i]\n",
    "\n",
    "val_features = preprocess_function(val_data) \n",
    "for i in range(len(val_data)):\n",
    "    for k in accepted_keys:\n",
    "        val_data[i][k] = val_features[k][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_keys = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "# 'input_ids', 'attention_mask', (#data, 4, #feature)\n",
    "train_features = [{k: v for k, v in train_data[i].items() if k in accepted_keys} for i in range(10)]\n",
    "batch = DataCollatorForMultipleChoice(tokenizer)(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] mark gave james the video tape of a tv advertisement, and mark told james that the advertisement was really good. mark already knew that james didn't see the tape. how did mark knew about that? [SEP] because the tape doesn't smell like the usual perfume james uses. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " \"[CLS] mark gave james the video tape of a tv advertisement, and mark told james that the advertisement was really good. mark already knew that james didn't see the tape. how did mark knew about that? [SEP] the video tape was so absurd that if james had seen it, he would've never told mark that it was really good [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " \"[CLS] mark gave james the video tape of a tv advertisement, and mark told james that the advertisement was really good. mark already knew that james didn't see the tape. how did mark knew about that? [SEP] because mark had a special psychic connection with james. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\",\n",
       " \"[CLS] mark gave james the video tape of a tv advertisement, and mark told james that the advertisement was really good. mark already knew that james didn't see the tape. how did mark knew about that? [SEP] none of above. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(batch[\"input_ids\"][-1][i].tolist()) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_predictions):\n",
    "    predictions, label_ids = eval_predictions\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (preds == label_ids).astype(np.float32).mean().item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 17:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.018414</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.825593</td>\n",
       "      <td>0.681319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.753733</td>\n",
       "      <td>0.703297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=0.9365406525440705, metrics={'train_runtime': 1060.1331, 'train_samples_per_second': 2.298, 'train_steps_per_second': 0.037, 'total_flos': 876275279863200.0, 'train_loss': 0.9365406525440705, 'epoch': 3.0})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert-base-uncased-finetuned/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_args = TrainingArguments(\n",
    "    output_dir = \"sample-test\",\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = batch_size,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "              model = model, \n",
    "              args = test_args, \n",
    "              compute_metrics = compute_metrics)\n",
    "test_results = trainer.predict(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
